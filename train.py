# -*- coding: utf-8 -*-
"""FORMATIVE 2-Deep Q

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qc9RBDS-6-R93GIncJD5YY9Usw-aoggk
"""

#!pip install stable-baselines3[extra] gymnasium[atari] ale-py

import gymnasium as gym
from stable_baselines3 import DQN
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import CheckpointCallback
import torch
import os
import ale_py

# Create logs directory
log_dir = "./logs/"
os.makedirs(log_dir, exist_ok=True)

# Ensure Stable-Baselines3 uses GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Train MLPPolicy model
print("Training MLPPolicy Model...")
mlp_env = gym.make("ALE/Pong-v5", render_mode="rgb_array")
mlp_env = Monitor(mlp_env, log_dir)

mlp_model = DQN("MlpPolicy", mlp_env, verbose=1, learning_rate=1e-4, gamma=0.99, tensorboard_log="./mlp_dqn_tensorboard/",device=device)
mlp_model.learn(total_timesteps=200000)  # Train for 200k steps
mlp_model.save("dqn_pong_mlp.zip")
mlp_env.close()

# Train CnnPolicy model
print("Training CnnPolicy Model...")
cnn_env = gym.make("ALE/Pong-v5", render_mode="rgb_array")
cnn_env = Monitor(cnn_env, log_dir)

cnn_model = DQN("CnnPolicy", cnn_env, verbose=1, buffer_size=1000000, learning_starts=10000, batch_size=16, gamma=0.99,
                learning_rate=1e-4, target_update_interval=1000, train_freq=(4, "step"), exploration_fraction=0.1,
                exploration_final_eps=0.01, tensorboard_log="./cnn_dqn_tensorboard/",device=device)

# Create checkpoint callback to save the CNN model periodically
checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=log_dir, name_prefix="dqn_checkpoint")

cnn_model.learn(total_timesteps=50000, log_interval=10, callback=checkpoint_callback)
cnn_model.save("dqn_model.zip")
cnn_env.close()

# Evaluate CNN model
eval_env = gym.make("ALE/Pong-v5", render_mode="rgb_array")
eval_env = Monitor(eval_env, log_dir)
mean_reward_cnn, std_reward_cnn = evaluate_policy(cnn_model, eval_env, n_eval_episodes=10)
print(f"CnnPolicy - Mean Reward: {mean_reward_cnn} Â± {std_reward_cnn}")
eval_env.close()